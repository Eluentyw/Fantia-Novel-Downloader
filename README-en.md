# Fantia Novel Downloader v1.1

## Overview

This is a Python script designed to download novel posts from the creator support platform "Fantia" and save them as local text files.

The tool utilizes a hybrid approach, combining HTML scraping with direct requests to the official Fantia API for stable and reliable data retrieval. All settings are managed in an external configuration file, allowing for safe and flexible operation without modifying the source code.

## Features

*   **Batch Processing**: Download novels from multiple fan clubs in a single run.
*   **Scope Filtering**: Choose to download "all," "paid only," or "free only" posts.
*   **External Configuration**: Authentication credentials and operational settings are managed in a `config.ini` file for safety and ease of maintenance.
*   **Organized Folder Structure**: Automatically creates subdirectories for each fan club to keep downloaded files organized.
*   **Robust Retrieval Logic**: Fetches the post list from HTML and post content from the API, making it resilient to minor site design changes.

## ⚠︎ Disclaimer

*   This tool should be used only for personal backup and educational purposes.
*   The author is not responsible for any issues that may arise from the use of this tool, including but not limited to account restrictions.
*   Please respect Fantia's Terms of Service. Use this tool at your own risk and avoid placing excessive load on their servers. Redistribution or sale of downloaded content may violate copyright laws.

---

## 1. Requirements

*   Python 3.6 or higher
*   Required Python libraries:
    *   `requests`
    *   `beautifulsoup4`

## 2. Setup

1.  **Clone or download the repository**
    ```bash
    git clone https://github.com/your-username/fantia-downloader.git
    cd fantia-downloader
    ```
    Alternatively, download the ZIP file and extract it to your desired location.

2.  **Install required libraries**
    Open a terminal or command prompt and run the following command:
    ```bash
    pip install requests beautifulsoup4
    ```

3.  **Prepare configuration files**
    *   In the same directory as `fantia_scraper.py`, manually create an empty text file named `DL_links.txt`.
    *   The `config.ini` file will be generated automatically on the first run.

## 3. Configuration

The application's behavior is controlled by two files: `config.ini` and `DL_links.txt`. **You do not need to edit the main script (`fantia_scraper.py`).**

### 3.1. `config.ini` Settings

This file is automatically generated on the first run. Open it and fill in the required information.

```ini
[Authentication]
user_agent = Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36
cookie = Please paste the Cookie string copied from your browser here
x_csrf_token = Please paste the X-Csrf-Token copied from your browser here

[Settings]
# Download scope: specify one of all, paid, or free
download_scope = all
# Root folder name for downloaded files
root_output_dir = fantia_novels
# Delay in seconds between requests. Recommended: 1.0 or higher to reduce server load
request_delay = 1.5
```

#### **How to Obtain Authentication Credentials (`cookie` and `x_csrf_token`)**

These are essential for proving to Fantia's server that you are a legitimate, logged-in user.

1.  Log in to Fantia in your PC web browser (Chrome is recommended).
2.  Navigate to **any novel post page that you have permission to view**.
3.  Press `F12` to open the Developer Tools and select the **"Network"** tab.
4.  In the filter bar, click on **"Fetch/XHR"**.
5.  Press `F5` to reload the page.
6.  In the list of requests, find and click on one that looks like `posts/1234567`.
7.  In the details pane on the right, find the **"Headers"** tab and scroll down to the **"Request Headers"** section.
8.  Copy the following two values and paste them into your `config.ini`:
    *   **`cookie`**: The **entire line** of text, which is very long and contains `_session_id=...`.
    *   **`x-csrf-token`**: The token string, which consists of random-looking characters.

> **Note:** These credentials may expire over time or after logging out. If the script stops working, repeat these steps to get the latest values.

### 3.2. `DL_links.txt` Settings

In this file, list the URLs of the **novel post list pages** for the fan clubs you want to download from. Place one URL per line.

**Example:**
```
https://fantia.jp/fanclubs/493635/posts?tag=%E5%B0%8F%E8%AA%AC
https://fantia.jp/fanclubs/12345/posts?tag=novel
https://fantia.jp/fanclubs/98765/posts
```
> **Tip:** The `?tag=...` part is necessary if a fan club uses a specific tag for its novels. If you want to download all posts regardless of tags, remove that part of the URL.

## 4. Usage

Once all configuration is complete, run the script from your terminal or command prompt:

```bash
python fantia_scraper.py
```

The program will start and process each URL listed in `DL_links.txt`.

## 5. Output Structure

The script will create the `root_output_dir` (default: `fantia_novels`) and generate subdirectories within it for each fan club.

```
.
├── fantia_scraper.py
├── config.ini
├── DL_links.txt
│
└── fantia_novels/
    ├── Fanclub A (Creator A)/
    │   ├── Novel Title 1.txt
    │   └── Novel Title 2.txt
    │
    └── Fanclub B (Creator B)/
        ├── Novel Title 3.txt
        └── ...
```

## 6. Troubleshooting

*   **"Login failed" error**:
    *   The `cookie` value in `config.ini` is likely incorrect or has expired. Follow the steps in "How to Obtain Authentication Credentials" to set the latest value.
*   **Errors occur while fetching individual posts**:
    *   The `x_csrf_token` in `config.ini` may have expired. Set the latest value, just like the `cookie`.
*   **No posts are found for a specific fan club**:
    *   Check the URL in `DL_links.txt`. Ensure it points to the correct post list page, including any specific tags the creator might be using for novels.

## 7. License

This project is licensed under the MIT License.
